# YOLO 模型部署

## 导出模型

```python
from ultralytics import YOLO

model = YOLO('best.pt')

# 导出为 ONNX
model.export(format='onnx')

# 导出为 TensorRT
model.export(format='engine')

# 导出为 TFLite（移动端）
model.export(format='tflite')

# 导出为 CoreML（iOS）
model.export(format='coreml')
```

## 支持的导出格式

| 格式 | 命令 | 用途 |
|------|------|------|
| ONNX | `format='onnx'` | 通用部署 |
| TensorRT | `format='engine'` | NVIDIA GPU 加速 |
| TFLite | `format='tflite'` | Android / 嵌入式 |
| CoreML | `format='coreml'` | iOS / macOS |
| NCNN | `format='ncnn'` | 移动端优化 |
| OpenVINO | `format='openvino'` | Intel 设备 |

## YOLO26 端到端导出

```python
model = YOLO('yolo26n.pt')

# 端到端模式（无NMS，更快）
model.export(format='onnx')  # 默认 end2end=True

# 传统模式（需要NMS）
model.export(format='onnx', end2end=False)
```

## Python 部署示例

```python
from ultralytics import YOLO
import cv2

model = YOLO('best.pt')

def detect(frame):
    results = model(frame, verbose=False)
    return results[0].plot()

# 实时检测
cap = cv2.VideoCapture(0)
while True:
    ret, frame = cap.read()
    if not ret:
        break

    annotated = detect(frame)
    cv2.imshow('Detection', annotated)

    if cv2.waitKey(1) == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

## 使用 ONNX Runtime

```python
import onnxruntime as ort
import numpy as np

session = ort.InferenceSession('best.onnx')
inputs = {session.get_inputs()[0].name: image_array}
outputs = session.run(None, inputs)
```

## 性能优化

| 方法 | 说明 |
|------|------|
| TensorRT | NVIDIA GPU 加速 2-5x |
| INT8 量化 | 减小模型大小，轻微精度损失 |
| FP16 半精度 | 平衡速度和精度 |
| 更小模型 | 使用 nano 或 small 版本 |
| 降低分辨率 | imgsz=416 或更小 |

## 边缘部署

YOLO26 专为边缘设备优化，CPU 速度提升 43%：

```python
model = YOLO('yolo26n.pt')

# 导出为 TFLite 用于移动端
model.export(format='tflite')

# 导出为 NCNN 用于嵌入式
model.export(format='ncnn')
```
