# AI é¡¹ç›®å®æˆ˜ï¼šå›¾åƒåˆ†ç±»ï¼ˆPyTorchï¼‰

æœ¬é¡¹ç›®å°†é€šè¿‡æ„å»ºä¸€ä¸ªæ‰‹å†™æ•°å­—è¯†åˆ«ç³»ç»Ÿï¼Œå¸®åŠ©ä½ æŒæ¡æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæŠ€èƒ½ã€‚

## é¡¹ç›®æ¦‚è¿°

æˆ‘ä»¬å°†ä½¿ç”¨ PyTorch å®ç°ï¼š
- âœ… CNN å·ç§¯ç¥ç»ç½‘ç»œ
- âœ… MNIST æ•°æ®é›†å¤„ç†
- âœ… æ¨¡å‹è®­ç»ƒä¸éªŒè¯
- âœ… æ¨¡å‹ä¿å­˜ä¸åŠ è½½
- âœ… é¢„æµ‹æ–°å›¾ç‰‡
- âœ… å¯è§†åŒ–ç»“æœ

## æŠ€æœ¯æ ˆ

- **PyTorch** - æ·±åº¦å­¦ä¹ æ¡†æ¶
- **torchvision** - è®¡ç®—æœºè§†è§‰åº“
- **matplotlib** - å¯è§†åŒ–
- **numpy** - æ•°å€¼è®¡ç®—
- **PIL** - å›¾åƒå¤„ç†

## ç¯å¢ƒå‡†å¤‡

### å®‰è£…ä¾èµ–

```bash
pip install torch torchvision matplotlib numpy pillow
```

éªŒè¯å®‰è£…ï¼š

```python
import torch
print(torch.__version__)  # åº”æ˜¾ç¤º 2.x æˆ–æ›´é«˜ç‰ˆæœ¬
print(torch.cuda.is_available())  # æ£€æŸ¥æ˜¯å¦æ”¯æŒ GPU
```

## é¡¹ç›®ç»“æ„

```
mnist-classifier/
â”œâ”€â”€ data/               # æ•°æ®é›†ç›®å½•ï¼ˆè‡ªåŠ¨ä¸‹è½½ï¼‰
â”œâ”€â”€ models/             # ä¿å­˜çš„æ¨¡å‹
â”œâ”€â”€ train.py            # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ predict.py          # é¢„æµ‹è„šæœ¬
â””â”€â”€ utils.py            # å·¥å…·å‡½æ•°
```

## ç¬¬ä¸€æ­¥ï¼šæ•°æ®å‡†å¤‡ä¸å¤„ç†

åˆ›å»º `utils.py`ï¼š

```python
import torch
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np

def get_data_loaders(batch_size=64):
    """
    è·å–è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨

    Args:
        batch_size: æ‰¹æ¬¡å¤§å°

    Returns:
        train_loader, test_loader
    """
    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),  # è½¬æ¢ä¸ºå¼ é‡
        transforms.Normalize((0.1307,), (0.3081,))  # æ ‡å‡†åŒ–ï¼ˆMNISTçš„å‡å€¼å’Œæ ‡å‡†å·®ï¼‰
    ])

    # ä¸‹è½½è®­ç»ƒé›†
    train_dataset = datasets.MNIST(
        root='./data',
        train=True,
        download=True,
        transform=transform
    )

    # ä¸‹è½½æµ‹è¯•é›†
    test_dataset = datasets.MNIST(
        root='./data',
        train=False,
        download=True,
        transform=transform
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False
    )

    return train_loader, test_loader

def visualize_batch(data_loader):
    """å¯è§†åŒ–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
    # è·å–ä¸€ä¸ªæ‰¹æ¬¡
    images, labels = next(iter(data_loader))

    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(2, 8, figsize=(15, 4))
    fig.suptitle('MNIST æ•°æ®æ ·æœ¬', fontsize=16)

    for i, ax in enumerate(axes.flat):
        if i < len(images):
            # æ˜¾ç¤ºå›¾åƒ
            ax.imshow(images[i].squeeze(), cmap='gray')
            ax.set_title(f'æ ‡ç­¾: {labels[i].item()}')
            ax.axis('off')

    plt.tight_layout()
    plt.savefig('mnist_samples.png', dpi=100, bbox_inches='tight')
    plt.show()

def plot_training_history(history):
    """
    ç»˜åˆ¶è®­ç»ƒå†å²

    Args:
        history: åŒ…å«è®­ç»ƒæŸå¤±å’Œå‡†ç¡®ç‡çš„å­—å…¸
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

    # ç»˜åˆ¶æŸå¤±
    ax1.plot(history['train_loss'], label='è®­ç»ƒæŸå¤±')
    ax1.plot(history['val_loss'], label='éªŒè¯æŸå¤±')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
    ax1.legend()
    ax1.grid(True)

    # ç»˜åˆ¶å‡†ç¡®ç‡
    ax2.plot(history['train_acc'], label='è®­ç»ƒå‡†ç¡®ç‡')
    ax2.plot(history['val_acc'], label='éªŒè¯å‡†ç¡®ç‡')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.savefig('training_history.png', dpi=100, bbox_inches='tight')
    plt.show()

def visualize_predictions(model, data_loader, device, num_samples=10):
    """
    å¯è§†åŒ–æ¨¡å‹é¢„æµ‹ç»“æœ

    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        data_loader: æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡ï¼ˆCPUæˆ–GPUï¼‰
        num_samples: æ˜¾ç¤ºçš„æ ·æœ¬æ•°é‡
    """
    model.eval()
    images, labels = next(iter(data_loader))

    # ç§»åŠ¨åˆ°è®¾å¤‡
    images = images.to(device)
    labels = labels.to(device)

    # é¢„æµ‹
    with torch.no_grad():
        outputs = model(images)
        _, predictions = torch.max(outputs, 1)

    # ç§»å›CPU
    images = images.cpu()
    labels = labels.cpu()
    predictions = predictions.cpu()

    # å¯è§†åŒ–
    fig, axes = plt.subplots(2, 5, figsize=(15, 6))
    fig.suptitle('æ¨¡å‹é¢„æµ‹ç»“æœ', fontsize=16)

    for i, ax in enumerate(axes.flat):
        if i < num_samples:
            ax.imshow(images[i].squeeze(), cmap='gray')

            # åˆ¤æ–­é¢„æµ‹æ˜¯å¦æ­£ç¡®
            color = 'green' if predictions[i] == labels[i] else 'red'

            ax.set_title(
                f'çœŸå®: {labels[i].item()}\né¢„æµ‹: {predictions[i].item()}',
                color=color
            )
            ax.axis('off')

    plt.tight_layout()
    plt.savefig('predictions.png', dpi=100, bbox_inches='tight')
    plt.show()
```

## ç¬¬äºŒæ­¥ï¼šæ„å»º CNN æ¨¡å‹

åˆ›å»º `train.py` ä¸­çš„æ¨¡å‹å®šä¹‰ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from tqdm import tqdm
import os
from utils import get_data_loaders, plot_training_history

class CNN(nn.Module):
    """
    å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹

    ç»“æ„:
    - å·ç§¯å±‚1: 1 -> 32 é€šé“
    - å·ç§¯å±‚2: 32 -> 64 é€šé“
    - å…¨è¿æ¥å±‚1: 64*5*5 -> 128
    - å…¨è¿æ¥å±‚2: 128 -> 10 (è¾“å‡º)
    """

    def __init__(self):
        super(CNN, self).__init__()

        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)
        self.pool1 = nn.MaxPool2d(2, 2)  # 28x28 -> 14x14
        self.dropout1 = nn.Dropout(0.25)

        # ç¬¬äºŒä¸ªå·ç§¯å—
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.pool2 = nn.MaxPool2d(2, 2)  # 14x14 -> 7x7
        self.dropout2 = nn.Dropout(0.25)

        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.dropout3 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = self.pool1(x)
        x = self.dropout1(x)

        # ç¬¬äºŒä¸ªå·ç§¯å—
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))
        x = self.pool2(x)
        x = self.dropout2(x)

        # å±•å¹³
        x = x.view(-1, 64 * 7 * 7)

        # å…¨è¿æ¥å±‚
        x = F.relu(self.fc1(x))
        x = self.dropout3(x)
        x = self.fc2(x)

        return x

def train_epoch(model, device, train_loader, optimizer, criterion, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    train_loss = 0
    correct = 0
    total = 0

    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')
    for batch_idx, (data, target) in enumerate(pbar):
        data, target = data.to(device), target.to(device)

        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)

        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()

        # ç»Ÿè®¡
        train_loss += loss.item()
        _, predicted = torch.max(output.data, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()

        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'Loss': f'{loss.item():.4f}',
            'Acc': f'{100.*correct/total:.2f}%'
        })

    avg_loss = train_loss / len(train_loader)
    accuracy = 100. * correct / total

    return avg_loss, accuracy

def validate(model, device, test_loader, criterion):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    test_loss = 0
    correct = 0

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()
            _, predicted = torch.max(output.data, 1)
            correct += (predicted == target).sum().item()

    test_loss /= len(test_loader)
    accuracy = 100. * correct / len(test_loader.dataset)

    print(f'\néªŒè¯é›†: å¹³å‡æŸå¤±: {test_loss:.4f}, å‡†ç¡®ç‡: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)')

    return test_loss, accuracy

def train(model, device, train_loader, test_loader, epochs=10, lr=0.001):
    """
    è®­ç»ƒæ¨¡å‹

    Args:
        model: æ¨¡å‹
        device: è®¾å¤‡
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        epochs: è®­ç»ƒè½®æ•°
        lr: å­¦ä¹ ç‡
    """
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=2, verbose=True
    )

    # è®­ç»ƒå†å²
    history = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': []
    }

    best_acc = 0

    for epoch in range(1, epochs + 1):
        print(f'\n{"="*50}')
        print(f'Epoch {epoch}/{epochs}')
        print(f'{"="*50}')

        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(
            model, device, train_loader, optimizer, criterion, epoch
        )

        # éªŒè¯
        val_loss, val_acc = validate(model, device, test_loader, criterion)

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(val_loss)

        # ä¿å­˜å†å²
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            os.makedirs('models', exist_ok=True)
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'accuracy': val_acc,
            }, 'models/best_model.pth')
            print(f'âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (å‡†ç¡®ç‡: {val_acc:.2f}%)')

    # ç»˜åˆ¶è®­ç»ƒå†å²
    plot_training_history(history)

    return history

def main():
    # è¶…å‚æ•°
    BATCH_SIZE = 64
    EPOCHS = 10
    LEARNING_RATE = 0.001

    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'ä½¿ç”¨è®¾å¤‡: {device}')

    # åŠ è½½æ•°æ®
    print('åŠ è½½æ•°æ®...')
    train_loader, test_loader = get_data_loaders(BATCH_SIZE)

    # åˆ›å»ºæ¨¡å‹
    print('åˆ›å»ºæ¨¡å‹...')
    model = CNN().to(device)
    print(model)

    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f'\næ€»å‚æ•°: {total_params:,}')
    print(f'å¯è®­ç»ƒå‚æ•°: {trainable_params:,}')

    # è®­ç»ƒæ¨¡å‹
    print('\nå¼€å§‹è®­ç»ƒ...')
    history = train(
        model, device, train_loader, test_loader,
        epochs=EPOCHS, lr=LEARNING_RATE
    )

    print('\nè®­ç»ƒå®Œæˆï¼')

if __name__ == '__main__':
    main()
```

## ç¬¬ä¸‰æ­¥ï¼šé¢„æµ‹è„šæœ¬

åˆ›å»º `predict.py`ï¼š

```python
import torch
import torch.nn.functional as F
from PIL import Image, ImageDraw, ImageFont
import numpy as np
from train import CNN

class DigitPredictor:
    """æ•°å­—é¢„æµ‹å™¨"""

    def __init__(self, model_path='models/best_model.pth'):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åŠ è½½æ¨¡å‹
        self.model = CNN().to(self.device)
        checkpoint = torch.load(model_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()

        print(f'æ¨¡å‹åŠ è½½æˆåŠŸ (å‡†ç¡®ç‡: {checkpoint["accuracy"]:.2f}%)')

    def preprocess_image(self, image_path):
        """
        é¢„å¤„ç†å›¾åƒ

        Args:
            image_path: å›¾åƒè·¯å¾„

        Returns:
            å¤„ç†åçš„å¼ é‡
        """
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('L')  # è½¬ä¸ºç°åº¦å›¾

        # è°ƒæ•´å¤§å°ä¸º28x28
        image = image.resize((28, 28), Image.Resampling.LANCZOS)

        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶å½’ä¸€åŒ–
        image_array = np.array(image, dtype=np.float32) / 255.0

        # æ ‡å‡†åŒ–ï¼ˆä½¿ç”¨MNISTçš„å‡å€¼å’Œæ ‡å‡†å·®ï¼‰
        image_array = (image_array - 0.1307) / 0.3081

        # è½¬æ¢ä¸ºå¼ é‡å¹¶æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        image_tensor = torch.from_numpy(image_array).unsqueeze(0).unsqueeze(0)

        return image_tensor.to(self.device)

    def predict(self, image_path):
        """
        é¢„æµ‹å›¾åƒä¸­çš„æ•°å­—

        Args:
            image_path: å›¾åƒè·¯å¾„

        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        # é¢„å¤„ç†
        image_tensor = self.preprocess_image(image_path)

        # é¢„æµ‹
        with torch.no_grad():
            output = self.model(image_tensor)
            probabilities = F.softmax(output, dim=1)
            confidence, predicted = torch.max(probabilities, 1)

        result = {
            'predicted_digit': predicted.item(),
            'confidence': confidence.item() * 100,
            'probabilities': {
                i: prob * 100
                for i, prob in enumerate(probabilities[0].cpu().numpy())
            }
        }

        return result

    def predict_with_visualization(self, image_path, output_path='prediction_result.png'):
        """
        é¢„æµ‹å¹¶å¯è§†åŒ–ç»“æœ

        Args:
            image_path: è¾“å…¥å›¾åƒè·¯å¾„
            output_path: è¾“å‡ºå›¾åƒè·¯å¾„
        """
        # é¢„æµ‹
        result = self.predict(image_path)

        # åŠ è½½åŸå§‹å›¾åƒ
        original_image = Image.open(image_path).convert('L')

        # åˆ›å»ºå¯è§†åŒ–
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))

        # æ˜¾ç¤ºåŸå§‹å›¾åƒ
        axes[0].imshow(original_image, cmap='gray')
        axes[0].set_title('è¾“å…¥å›¾åƒ', fontsize=14)
        axes[0].axis('off')

        # æ˜¾ç¤ºæ¦‚ç‡åˆ†å¸ƒ
        digits = list(result['probabilities'].keys())
        probs = list(result['probabilities'].values())

        bars = axes[1].bar(digits, probs, color='skyblue')

        # é«˜äº®é¢„æµ‹ç»“æœ
        bars[result['predicted_digit']].set_color('red')

        axes[1].set_xlabel('æ•°å­—', fontsize=12)
        axes[1].set_ylabel('æ¦‚ç‡ (%)', fontsize=12)
        axes[1].set_title(
            f'é¢„æµ‹ç»“æœ: {result["predicted_digit"]} (ç½®ä¿¡åº¦: {result["confidence"]:.2f}%)',
            fontsize=14
        )
        axes[1].set_ylim([0, 100])
        axes[1].grid(axis='y', alpha=0.3)

        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
        for bar, prob in zip(bars, probs):
            height = bar.get_height()
            axes[1].text(
                bar.get_x() + bar.get_width()/2.,
                height,
                f'{prob:.1f}%',
                ha='center',
                va='bottom',
                fontsize=9
            )

        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.show()

        print(f'\né¢„æµ‹ç»“æœ:')
        print(f'æ•°å­—: {result["predicted_digit"]}')
        print(f'ç½®ä¿¡åº¦: {result["confidence"]:.2f}%')
        print(f'\næ‰€æœ‰æ¦‚ç‡:')
        for digit, prob in result['probabilities'].items():
            print(f'{digit}: {prob:.2f}%')

        return result

def create_test_digit(digit, save_path='test_digit.png'):
    """
    åˆ›å»ºä¸€ä¸ªæµ‹è¯•ç”¨çš„æ‰‹å†™æ•°å­—å›¾åƒ

    Args:
        digit: è¦åˆ›å»ºçš„æ•°å­— (0-9)
        save_path: ä¿å­˜è·¯å¾„
    """
    from PIL import Image, ImageDraw, ImageFont

    # åˆ›å»ºç™½è‰²èƒŒæ™¯å›¾åƒ
    image = Image.new('L', (280, 280), color=255)
    draw = ImageDraw.Draw(image)

    # å°è¯•ä½¿ç”¨å¤§å­—ä½“ç»˜åˆ¶æ•°å­—
    try:
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 200)
    except:
        font = ImageFont.load_default()

    # ç»˜åˆ¶æ•°å­—ï¼ˆé»‘è‰²ï¼‰
    text = str(digit)
    bbox = draw.textbbox((0, 0), text, font=font)
    text_width = bbox[2] - bbox[0]
    text_height = bbox[3] - bbox[1]

    position = ((280 - text_width) // 2, (280 - text_height) // 2)
    draw.text(position, text, fill=0, font=font)

    # ä¿å­˜å›¾åƒ
    image.save(save_path)
    print(f'æµ‹è¯•æ•°å­—å›¾åƒå·²ä¿å­˜åˆ°: {save_path}')

def main():
    """ä¸»å‡½æ•°"""
    import matplotlib.pyplot as plt

    # åˆ›å»ºé¢„æµ‹å™¨
    predictor = DigitPredictor()

    # åˆ›å»ºæµ‹è¯•æ•°å­—
    print('\nåˆ›å»ºæµ‹è¯•æ•°å­—...')
    create_test_digit(7, 'test_digit.png')

    # é¢„æµ‹å¹¶å¯è§†åŒ–
    print('\næ­£åœ¨é¢„æµ‹...')
    predictor.predict_with_visualization('test_digit.png')

if __name__ == '__main__':
    main()
```

## ä½¿ç”¨æ•™ç¨‹

### 1. è®­ç»ƒæ¨¡å‹

```bash
python train.py
```

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šçœ‹åˆ°ï¼š
- æ¯ä¸ªepochçš„æŸå¤±å’Œå‡†ç¡®ç‡
- è¿›åº¦æ¡æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
- è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
- ç”Ÿæˆè®­ç»ƒå†å²å›¾è¡¨

### 2. é¢„æµ‹æ–°å›¾ç‰‡

```bash
python predict.py
```

ä¼šç”Ÿæˆï¼š
- æµ‹è¯•æ•°å­—å›¾åƒ
- é¢„æµ‹ç»“æœå¯è§†åŒ–
- æ§åˆ¶å°è¾“å‡ºè¯¦ç»†é¢„æµ‹ä¿¡æ¯

### 3. ä½¿ç”¨è‡ªå·±çš„å›¾ç‰‡

```python
from predict import DigitPredictor

# åˆ›å»ºé¢„æµ‹å™¨
predictor = DigitPredictor('models/best_model.pth')

# é¢„æµ‹
result = predictor.predict_with_visualization('your_image.png')
```

## æ ¸å¿ƒæ¦‚å¿µè®²è§£

### 1. CNN æ¶æ„

```
è¾“å…¥å›¾åƒ (1x28x28)
    â†“
Conv2d(1â†’32) + ReLU + Conv2d(32â†’32) + ReLU + MaxPool
    â†“ (14x14x32)
Conv2d(32â†’64) + ReLU + Conv2d(64â†’64) + ReLU + MaxPool
    â†“ (7x7x64)
Flatten + FC(64*7*7â†’128) + ReLU + Dropout
    â†“ (128)
FC(128â†’10) + Softmax
    â†“ (10ç±»æ¦‚ç‡)
```

### 2. æ•°æ®å¢å¼º

```python
transform = transforms.Compose([
    transforms.RandomRotation(10),  # éšæœºæ—‹è½¬
    transforms.RandomAffine(0, translate=(0.1, 0.1)),  # éšæœºå¹³ç§»
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])
```

### 3. æ­£åˆ™åŒ–æŠ€æœ¯

- **Dropout**: é˜²æ­¢è¿‡æ‹Ÿåˆ
- **æ•°æ®å¢å¼º**: å¢åŠ è®­ç»ƒæ•°æ®å¤šæ ·æ€§
- **æƒé‡è¡°å‡**: L2æ­£åˆ™åŒ–
- **æ—©åœ**: é˜²æ­¢è¿‡è®­ç»ƒ

## æ€§èƒ½ä¼˜åŒ–

### 1. ä½¿ç”¨ GPU

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
```

### 2. æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    output = model(data)
    loss = criterion(output, target)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 3. æ•°æ®åŠ è½½ä¼˜åŒ–

```python
DataLoader(
    dataset,
    batch_size=64,
    num_workers=4,  # å¤šè¿›ç¨‹åŠ è½½
    pin_memory=True  # åŠ é€ŸGPUä¼ è¾“
)
```

## å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒæ—¶æ˜¾å­˜ä¸è¶³
**A**: å‡å°batch_sizeæˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹

### Q2: æ¨¡å‹è¿‡æ‹Ÿåˆ
**A**:
- å¢åŠ Dropout
- ä½¿ç”¨æ•°æ®å¢å¼º
- å‡å°æ¨¡å‹å¤æ‚åº¦

### Q3: è®­ç»ƒé€Ÿåº¦æ…¢
**A**:
- ä½¿ç”¨GPU
- å¢åŠ num_workers
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

## æ‰©å±•é¡¹ç›®

å®ŒæˆåŸºç¡€ç‰ˆæœ¬åï¼Œå°è¯•ï¼š

1. **æ”¹è¿›æ¨¡å‹** - ResNetã€VGGã€EfficientNet
2. **æ•°æ®å¢å¼º** - æ·»åŠ æ›´å¤šå¢å¼ºæŠ€æœ¯
3. **è¿ç§»å­¦ä¹ ** - ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
4. **æ³¨æ„åŠ›æœºåˆ¶** - æ·»åŠ SEæ¨¡å—ã€CBAM
5. **æ¨¡å‹å‹ç¼©** - å‰ªæã€é‡åŒ–ã€çŸ¥è¯†è’¸é¦

## å­¦ä¹ èµ„æº

- [PyTorch å®˜æ–¹æ•™ç¨‹](https://pytorch.org/tutorials/)
- [CS231n: CNN for Visual Recognition](http://cs231n.stanford.edu/)
- [åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ](https://zh.d2l.ai/)

ç¥ä½ å­¦ä¹ æ„‰å¿«ï¼ğŸš€
